{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Shot Target Interaction Module\n",
    "\n",
    "This module loads generated adversarial prompts and sends them to target LLMs, capturing responses and reasoning traces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import uuid\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from typing import Dict, List, Optional, Union, Any, Tuple\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs('./logs/single_shot', exist_ok=True)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Target Model Configuration\n",
    "# Set your target model endpoint and API key here\n",
    "TARGET_MODEL_BASE_URL = os.getenv(\"TARGET_MODEL_BASE_URL\", \"http://localhost:1234/v1\")\n",
    "TARGET_MODEL_API_KEY = os.getenv(\"TARGET_MODEL_API_KEY\", \"not-needed\")\n",
    "TARGET_MODEL_NAME = os.getenv(\"TARGET_MODEL_NAME\", \"local-model\")\n",
    "\n",
    "# Initialize OpenAI client for target model\n",
    "target_client = openai.OpenAI(\n",
    "    base_url=TARGET_MODEL_BASE_URL,\n",
    "    api_key=TARGET_MODEL_API_KEY\n",
    ")\n",
    "\n",
    "print(f\"Target model endpoint configured: {TARGET_MODEL_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Generated Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_generated_prompts(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load generated adversarial prompts from CSV or JSON file\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the generated prompts file\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing the prompts\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.json'):\n",
    "            df = pd.read_json(file_path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file format. Please use CSV or JSON.\")\n",
    "        \n",
    "        print(f\"Successfully loaded {len(df)} prompts from {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading prompts: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Load generated prompts\n",
    "prompts_file = \"./generated_prompts/sample_prompts.csv\"  # Update with your actual path\n",
    "prompts_df = load_generated_prompts(prompts_file)\n",
    "\n",
    "if not prompts_df.empty:\n",
    "    print(f\"Loaded {len(prompts_df)} prompts\")\n",
    "    print(\"Columns:\", list(prompts_df.columns))\n",
    "    prompts_df.head()\n",
    "else:\n",
    "    print(\"No prompts loaded. Please check the file path and format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Single-Shot Target Interaction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_shot_interaction(prompt_data: Dict, target_model_config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Send a single prompt to target LLM and capture response with reasoning trace\n",
    "    \n",
    "    Args:\n",
    "        prompt_data (dict): Dictionary containing prompt information\n",
    "        target_model_config (dict): Configuration for the target model API\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains prompt, response, reasoning trace, and metadata\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract prompt information\n",
    "        prompt_text = prompt_data.get('prompt', '')\n",
    "        scenario = prompt_data.get('scenario', 'Unknown')\n",
    "        prompt_id = prompt_data.get('id', str(uuid.uuid4()))\n",
    "        \n",
    "        # Prepare messages for target model\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ]\n",
    "        \n",
    "        # Add system prompt if provided\n",
    "        if target_model_config.get('system_prompt'):\n",
    "            messages.insert(0, {\"role\": \"system\", \"content\": target_model_config['system_prompt']})\n",
    "        \n",
    "        # Send request to target model\n",
    "        response = target_client.chat.completions.create(\n",
    "            model=target_model_config.get('model', TARGET_MODEL_NAME),\n",
    "            messages=messages,\n",
    "            temperature=target_model_config.get('temperature', 0.7),\n",
    "            max_tokens=target_model_config.get('max_tokens', 500)\n",
    "        )\n",
    "        \n",
    "        # Extract response content\n",
    "        response_content = response.choices[0].message.content\n",
    "        \n",
    "        # Try to extract reasoning trace if available\n",
    "        reasoning_trace = None\n",
    "        if hasattr(response.choices[0].message, 'reasoning_trace'):\n",
    "            reasoning_trace = response.choices[0].message.reasoning_trace\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            \"interaction_id\": str(uuid.uuid4()),\n",
    "            \"prompt_id\": prompt_id,\n",
    "            \"scenario\": scenario,\n",
    "            \"attacker_prompt\": prompt_text,\n",
    "            \"target_response\": response_content,\n",
    "            \"target_reasoning\": reasoning_trace,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"target_model\": target_model_config.get('model', TARGET_MODEL_NAME),\n",
    "            \"model_provider\": target_model_config.get('provider', 'unknown'),\n",
    "            \"status\": \"success\"\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in single-shot interaction for prompt {prompt_data.get('id', 'unknown')}: {e}\")\n",
    "        return {\n",
    "            \"interaction_id\": str(uuid.uuid4()),\n",
    "            \"prompt_id\": prompt_data.get('id', 'unknown'),\n",
    "            \"scenario\": prompt_data.get('scenario', 'Unknown'),\n",
    "            \"attacker_prompt\": prompt_data.get('prompt', ''),\n",
    "            \"target_response\": None,\n",
    "            \"target_reasoning\": None,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.datetime.now().isoformat(),\n",
    "            \"target_model\": target_model_config.get('model', TARGET_MODEL_NAME),\n",
    "            \"model_provider\": target_model_config.get('provider', 'unknown'),\n",
    "            \"status\": \"error\"\n",
    "        }\n",
    "\n",
    "# Test the function with one prompt\n",
    "if not prompts_df.empty:\n",
    "    # Get first prompt as test\n",
    "    test_prompt = prompts_df.iloc[0].to_dict()\n",
    "    \n",
    "    # Define target model configuration\n",
    "    target_model_config = {\n",
    "        \"provider\": \"lmstudio\",  # or \"openai\", \"ollama\"\n",
    "        \"api_key\": TARGET_MODEL_API_KEY,\n",
    "        \"base_url\": TARGET_MODEL_BASE_URL,\n",
    "        \"model\": TARGET_MODEL_NAME,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "    \n",
    "    # Run single-shot interaction\n",
    "    test_result = run_single_shot_interaction(test_prompt, target_model_config)\n",
    "    print(\"Test interaction result:\")\n",
    "    print(json.dumps(test_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process All Generated Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_prompts(prompts_df: pd.DataFrame, target_model_config: Dict) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Process all generated prompts through single-shot interactions\n",
    "    \n",
    "    Args:\n",
    "        prompts_df (pd.DataFrame): DataFrame containing generated prompts\n",
    "        target_model_config (dict): Configuration for the target model API\n",
    "    \n",
    "    Returns:\n",
    "        list: List of interaction results\n",
    "    \"\"\"\n",
    "    interaction_results = []\n",
    "    \n",
    "    if prompts_df.empty:\n",
    "        print(\"No prompts to process.\")\n",
    "        return interaction_results\n",
    "    \n",
    "    print(f\"Processing {len(prompts_df)} prompts...\")\n",
    "    \n",
    "    # Process each prompt\n",
    "    for idx, row in prompts_df.iterrows():\n",
    "        prompt_data = row.to_dict()\n",
    "        \n",
    "        print(f\"Processing prompt {idx+1}/{len(prompts_df)}: {prompt_data.get('scenario', 'Unknown')}\")\n",
    "        \n",
    "        # Run single-shot interaction\n",
    "        result = run_single_shot_interaction(prompt_data, target_model_config)\n",
    "        interaction_results.append(result)\n",
    "        \n",
    "        # Add a small delay to avoid overwhelming the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(f\"Completed processing {len(interaction_results)} prompts.\")\n",
    "    return interaction_results\n",
    "\n",
    "# Process all prompts\n",
    "if not prompts_df.empty:\n",
    "    # Define target model configuration\n",
    "    target_model_config = {\n",
    "        \"provider\": \"lmstudio\",  # or \"openai\", \"ollama\"\n",
    "        \"api_key\": TARGET_MODEL_API_KEY,\n",
    "        \"base_url\": TARGET_MODEL_BASE_URL,\n",
    "        \"model\": TARGET_MODEL_NAME,\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 500\n",
    "    }\n",
    "    \n",
    "    # Process all prompts\n",
    "    all_results = process_all_prompts(prompts_df, target_model_config)\n",
    "    print(f\"Processed {len(all_results)} prompts.\")\n",
    "    \n",
    "    # Show summary\n",
    "    if all_results:\n",
    "        successful_interactions = [r for r in all_results if r.get('status') == 'success']\n",
    "        errored_interactions = [r for r in all_results if r.get('status') == 'error']\n",
    "        \n",
    "        print(f\"Successful interactions: {len(successful_interactions)}\")\n",
    "        print(f\"Errored interactions: {len(errored_interactions)}\")\n",
    "        \n",
    "        # Show sample successful result\n",
    "        if successful_interactions:\n",
    "            sample = successful_interactions[0]\n",
    "            print(\"\\nSample successful interaction:\")\n",
    "            print(f\"  Scenario: {sample.get('scenario', 'Unknown')}\")\n",
    "            print(f\"  Prompt: {sample.get('attacker_prompt', '')[:100]}...\")\n",
    "            print(f\"  Response length: {len(sample.get('target_response', ''))} characters\")\n",
    "else:\n",
    "    print(\"No prompts to process.\")\n",
    "    all_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_interaction_results(results: List[Dict], format: str = \"json\"):\n",
    "    \"\"\"\n",
    "    Export interaction results to JSON or CSV format\n",
    "    \n",
    "    Args:\n",
    "        results (list): List of interaction results\n",
    "        format (str): Export format (\"json\" or \"csv\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        \n",
    "        if format.lower() == \"json\":\n",
    "            filename = f\"./logs/single_shot/interaction_results_{timestamp}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"Exported {len(results)} results to {filename}\")\n",
    "            return filename\n",
    "        \n",
    "        elif format.lower() == \"csv\":\n",
    "            filename = f\"./logs/single_shot/interaction_results_{timestamp}.csv\"\n",
    "            \n",
    "            # Flatten data for CSV\n",
    "            csv_data = []\n",
    "            for result in results:\n",
    "                row = {\n",
    "                    \"interaction_id\": result.get(\"interaction_id\", \"\"),\n",
    "                    \"prompt_id\": result.get(\"prompt_id\", \"\"),\n",
    "                    \"scenario\": result.get(\"scenario\", \"\"),\n",
    "                    \"attacker_prompt\": result.get(\"attacker_prompt\", \"\"),\n",
    "                    \"target_response\": result.get(\"target_response\", \"\"),\n",
    "                    \"target_reasoning\": str(result.get(\"target_reasoning\", \"\")),\n",
    "                    \"timestamp\": result.get(\"timestamp\", \"\"),\n",
    "                    \"target_model\": result.get(\"target_model\", \"\"),\n",
    "                    \"model_provider\": result.get(\"model_provider\", \"\"),\n",
    "                    \"status\": result.get(\"status\", \"\"),\n",
    "                    \"error\": result.get(\"error\", \"\")\n",
    "                }\n",
    "                csv_data.append(row)\n",
    "            \n",
    "            if csv_data:\n",
    "                df = pd.DataFrame(csv_data)\n",
    "                df.to_csv(filename, index=False)\n",
    "                print(f\"Exported {len(csv_data)} results to {filename}\")\n",
    "            else:\n",
    "                # Create empty CSV with headers\n",
    "                headers = [\"interaction_id\", \"prompt_id\", \"scenario\", \"attacker_prompt\", \n",
    "                          \"target_response\", \"target_reasoning\", \"timestamp\", \n",
    "                          \"target_model\", \"model_provider\", \"status\", \"error\"]\n",
    "                with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(headers)\n",
    "                print(f\"Created empty CSV file with headers: {filename}\")\n",
    "            \n",
    "            return filename\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported format: {format}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting results: {e}\")\n",
    "        return None\n",
    "\n",
    "# Export results\n",
    "if all_results:\n",
    "    json_file = export_interaction_results(all_results, \"json\")\n",
    "    csv_file = export_interaction_results(all_results, \"csv\")\n",
    "    \n",
    "    print(\"\\nExport completed!\")\n",
    "    print(f\"JSON file: {json_file}\")\n",
    "    print(f\"CSV file: {csv_file}\")\n",
    "else:\n",
    "    print(\"No results to export.\")\n",
    "    # Create empty files\n",
    "    export_interaction_results([], \"json\")\n",
    "    export_interaction_results([], \"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Single-Shot Target Interaction Complete ===\")\n",
    "print(f\"Total prompts processed: {len(prompts_df) if not prompts_df.empty else 0}\")\n",
    "print(f\"Total interactions completed: {len(all_results) if 'all_results' in locals() else 0}\")\n",
    "\n",
    "# Show results summary if available\n",
    "if 'all_results' in locals() and all_results:\n",
    "    successful = [r for r in all_results if r.get('status') == 'success']\n",
    "    errored = [r for r in all_results if r.get('status') == 'error']\n",
    "    \n",
    "    print(f\"Successful interactions: {len(successful)}\")\n",
    "    print(f\"Errored interactions: {len(errored)}\")\n",
    "    \n",
    "    # Count by scenario\n",
    "    scenario_counts = {}\n",
    "    for result in all_results:\n",
    "        scenario = result.get('scenario', 'Unknown')\n",
    "        scenario_counts[scenario] = scenario_counts.get(scenario, 0) + 1\n",
    "    \n",
    "    print(\"\\nInteraction count by scenario:\")\n",
    "    for scenario, count in sorted(scenario_counts.items()):\n",
    "        print(f\"  {scenario}: {count}\")\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Review the exported JSON and CSV files in ./logs/single_shot/\")\n",
    "print(\"2. Analyze target responses for safety compliance and vulnerability patterns\")\n",
    "print(\"3. Use the results for judge LLM evaluation\")\n",
    "print(\"4. Identify areas for improved safety mechanisms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}